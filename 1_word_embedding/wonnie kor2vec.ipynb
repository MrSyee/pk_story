{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나무위키 덤프 + 포켓몬 크롤링 데이터 Kor2Vec입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from konlpy.tag import Twitter\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(train_text, min_count, sampling_rate):\n",
    "    words = list()\n",
    "    \n",
    "    fname= './data/namu_wonnie.txt'\n",
    "    \n",
    "    with open(fname, 'r') as f:\n",
    "        namu = [data.split() for data in f]\n",
    "        \n",
    "    print(len(namu))\n",
    "    \n",
    "    for data in namu:\n",
    "        words.append(data)\n",
    "        \n",
    "    for line in desc_list:\n",
    "        sentence = re.sub(r\"[^ㄱ-힣a-zA-Z0-9]+\", ' ', line).strip().split()\n",
    "        if sentence:\n",
    "            words.append(sentence)\n",
    "\n",
    "    word_counter = [['UNK', -1]] #시작점\n",
    "    word_counter.extend(collections.Counter([word for sentence in words for word in sentence]).most_common())\n",
    "    word_counter = [item for item in word_counter if item[1] >= min_count or item[0] == 'UNK']\n",
    "\n",
    "    word_list = list()\n",
    "    word_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        word_list.append(word) # 학습에 사용된 word를 저장한다. (visualize를 위해)\n",
    "        word_dict[word] = len(word_dict)\n",
    "    word_reverse_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    word_to_pos_li = dict()\n",
    "    pos_list = list()\n",
    "    twitter = Twitter()\n",
    "    for w in word_dict:\n",
    "        w_pos_li = list()\n",
    "        for pos in twitter.pos(w, norm=True):\n",
    "            w_pos_li.append(pos)\n",
    "\n",
    "        word_to_pos_li[word_dict[w]] = w_pos_li\n",
    "        pos_list += w_pos_li\n",
    "\n",
    "    pos_counter = collections.Counter(pos_list).most_common()\n",
    "\n",
    "    pos_dict = dict()\n",
    "    for pos, _ in pos_counter:\n",
    "        pos_dict[pos] = len(pos_dict)\n",
    "\n",
    "    pos_reverse_dict = dict(zip(pos_dict.values(), pos_dict.keys()))\n",
    "\n",
    "    word_to_pos_dict = dict()\n",
    "\n",
    "    for word_id, pos_li in word_to_pos_li.items():\n",
    "        pos_id_li = list()\n",
    "        for pos in pos_li:\n",
    "            pos_id_li.append(pos_dict[pos])\n",
    "        word_to_pos_dict[word_id] = pos_id_li\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for sentence in words:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "            else:\n",
    "                index = word_dict['UNK']\n",
    "                unk_count += 1\n",
    "            s.append(index)\n",
    "        data.append(s)\n",
    "    word_counter[0][1] = max(1, unk_count)\n",
    "\n",
    "    # data = sub_sampling(data, word_counter, word_dict, sampling_rate)\n",
    "\n",
    "    return data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict, word_list\n",
    "\n",
    "def sub_sampling(data, word_counter, word_dict, sampling_rate):\n",
    "    total_words = sum([len(sentence) for sentence in data])\n",
    "    # print(\"total_words: {}\".format(total_words))\n",
    "    prob_dict = dict()\n",
    "    for word, count in word_counter:\n",
    "        f = count / total_words # 빈도수가 많을수록 f가 1에 가까워짐.\n",
    "        p = max(0, 1 - math.sqrt(sampling_rate / f)) # sampling_rate가 0.0001이면 f가 클수록 prob이 커진다.\n",
    "        prob_dict[word_dict[word]] = p\n",
    "        # print(\"count : {}, f : {}, p : {}, prob_dict : {}\".format(count, f, p, prob_dict))\n",
    "\n",
    "    new_data = list()\n",
    "    for sentence in data:\n",
    "        s = list()\n",
    "        for word in sentence:\n",
    "            prob = prob_dict[word]\n",
    "            if random.random() > prob: # prob이 작을수록 s에 저장되기 쉬움.\n",
    "                s.append(word)\n",
    "        new_data.append(s)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460448\n"
     ]
    }
   ],
   "source": [
    "# crawling한 데이터를 불러온다.\n",
    "pk_data = pd.read_csv(DATA_PATH + 'pk_data_g1.csv')\n",
    "desc_list = []\n",
    "for i in range(len(pk_data)):\n",
    "    for desc in pk_data['desc'][i].split('.'):\n",
    "        desc_list.append(desc)\n",
    "\n",
    "sampling_rate = 0.0001\n",
    "min_count = 5\n",
    "\n",
    "data, word_dict, word_reverse_dict, pos_dict, pos_reverse_dict, word_to_pos_dict, word_list \\\n",
    "        = build_dataset(desc_list, min_count, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용된 word list 저장\n",
    "f = open(\"word_list.txt\", 'w')\n",
    "for word in word_list:\n",
    "    input_word = \"{} \".format(word)\n",
    "    f.write(input_word)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences : 465247\n",
      "vocabulary size : 94749\n",
      "pos size : 33304\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(word_dict)\n",
    "pos_size = len(pos_dict)\n",
    "num_sentences = len(data)\n",
    "\n",
    "print(\"number of sentences :\", num_sentences)\n",
    "print(\"vocabulary size :\", vocabulary_size)\n",
    "print(\"pos size :\", pos_size)\n",
    "\n",
    "pos_li = []\n",
    "for key in sorted(pos_reverse_dict):\n",
    "    pos_li.append(pos_reverse_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "batch_size = 150\n",
    "\n",
    "# kor2vec 의 input index list와 output index list를 만든다.\n",
    "# 윈도우 사이즈에 따라 input output pair가 늘어난다.(input이 중복)\n",
    "def generate_input_output_list(data, window_size):\n",
    "    input_li = list()\n",
    "    output_li = list()\n",
    "    for sentence in data:\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    if sentence[i]!=word_dict['UNK'] and sentence[j]!=word_dict['UNK']:\n",
    "                        input_li.append(sentence[i])\n",
    "                        output_li.append(sentence[j])\n",
    "    return input_li, output_li\n",
    "\n",
    "input_li, output_li = generate_input_output_list(data, window_size)\n",
    "input_li_size = len(input_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "20073212\n",
      "(150,)\n",
      "[ 1969  2533     1     1     1  6181  6181  6181     8     8     8 39568\n",
      " 39568 39568 27947 27947  1732  1732 22535 22535    54 59115    54 67444\n",
      "    54 18937 39568 39568 39568 39568 39568 43173 43173 43173 43173 43173\n",
      " 59116 59116 59116 59116 59116 59116 67445 67445 67445 67445 67445 67445\n",
      "  4149  4149  4149  4149  4149  4149  4149  4536  4536  4536  4536  4536\n",
      "  4536  4536  4199  4199  4199  4199  4199 36490 36490 36490 36490 36490\n",
      "  2175  2175  2175  2175 12893 12893 12893 12893    21    21    21    21\n",
      " 21477 21477 21477 21477   799   799   799   799   799  1642  1642  1642\n",
      "  1642  1642   251   251   251   251   251   251  3954  3954  3954  3954\n",
      "  3954  3954  3954  7538  7538  7538  7538  7538  7538  7538 67446 67446\n",
      " 67446 67446 67446 67446 67446 67446 18201 18201 18201 18201 18201 18201\n",
      " 18201 18201 10157 10157 10157 10157 10157 10157 10157 10157 10157 15119\n",
      " 15119 15119 15119 15119 15119 15119]\n",
      "(150, 1)\n",
      "[[ 2533]\n",
      " [ 1969]\n",
      " [ 6181]\n",
      " [    8]\n",
      " [39568]\n",
      " [    1]\n",
      " [    8]\n",
      " [39568]\n",
      " [    1]\n",
      " [ 6181]\n",
      " [39568]\n",
      " [    1]\n",
      " [ 6181]\n",
      " [    8]\n",
      " [ 1732]\n",
      " [22535]\n",
      " [27947]\n",
      " [22535]\n",
      " [27947]\n",
      " [ 1732]\n",
      " [59115]\n",
      " [   54]\n",
      " [67444]\n",
      " [   54]\n",
      " [18937]\n",
      " [   54]\n",
      " [43173]\n",
      " [59116]\n",
      " [67445]\n",
      " [ 4149]\n",
      " [ 4536]\n",
      " [39568]\n",
      " [59116]\n",
      " [67445]\n",
      " [ 4149]\n",
      " [ 4536]\n",
      " [39568]\n",
      " [43173]\n",
      " [67445]\n",
      " [ 4149]\n",
      " [ 4536]\n",
      " [ 4199]\n",
      " [39568]\n",
      " [43173]\n",
      " [59116]\n",
      " [ 4149]\n",
      " [ 4536]\n",
      " [ 4199]\n",
      " [39568]\n",
      " [43173]\n",
      " [59116]\n",
      " [67445]\n",
      " [ 4536]\n",
      " [ 4199]\n",
      " [36490]\n",
      " [39568]\n",
      " [43173]\n",
      " [59116]\n",
      " [67445]\n",
      " [ 4149]\n",
      " [ 4199]\n",
      " [36490]\n",
      " [59116]\n",
      " [67445]\n",
      " [ 4149]\n",
      " [ 4536]\n",
      " [36490]\n",
      " [ 4149]\n",
      " [ 4536]\n",
      " [ 4199]\n",
      " [ 2175]\n",
      " [12893]\n",
      " [36490]\n",
      " [12893]\n",
      " [   21]\n",
      " [21477]\n",
      " [36490]\n",
      " [ 2175]\n",
      " [   21]\n",
      " [21477]\n",
      " [ 2175]\n",
      " [12893]\n",
      " [21477]\n",
      " [  799]\n",
      " [ 2175]\n",
      " [12893]\n",
      " [   21]\n",
      " [  799]\n",
      " [   21]\n",
      " [21477]\n",
      " [ 1642]\n",
      " [  251]\n",
      " [ 3954]\n",
      " [  799]\n",
      " [  251]\n",
      " [ 3954]\n",
      " [ 7538]\n",
      " [67446]\n",
      " [  799]\n",
      " [ 1642]\n",
      " [ 3954]\n",
      " [ 7538]\n",
      " [67446]\n",
      " [18201]\n",
      " [  799]\n",
      " [ 1642]\n",
      " [  251]\n",
      " [ 7538]\n",
      " [67446]\n",
      " [18201]\n",
      " [10157]\n",
      " [ 1642]\n",
      " [  251]\n",
      " [ 3954]\n",
      " [67446]\n",
      " [18201]\n",
      " [10157]\n",
      " [15119]\n",
      " [ 1642]\n",
      " [  251]\n",
      " [ 3954]\n",
      " [ 7538]\n",
      " [18201]\n",
      " [10157]\n",
      " [15119]\n",
      " [ 1153]\n",
      " [  251]\n",
      " [ 3954]\n",
      " [ 7538]\n",
      " [67446]\n",
      " [10157]\n",
      " [15119]\n",
      " [ 1153]\n",
      " [   36]\n",
      " [ 3954]\n",
      " [ 7538]\n",
      " [67446]\n",
      " [18201]\n",
      " [15119]\n",
      " [ 1153]\n",
      " [   36]\n",
      " [21478]\n",
      " [  112]\n",
      " [ 7538]\n",
      " [67446]\n",
      " [18201]\n",
      " [10157]\n",
      " [ 1153]\n",
      " [   36]\n",
      " [21478]]\n",
      "[[134, 24, 1897], [755, 21, 3], [820], [820], [820], [968, 687], [968, 687], [968, 687], [24], [24], [24], [2778, 5547], [2778, 5547], [2778, 5547], [1327, 67], [1327, 67], [7192], [7192], [65, 257], [65, 257], [93], [134, 24, 4187, 3, 3685, 24985, 4187, 24986], [93], [752, 3], [93], [2809, 8], [2778, 5547], [2778, 5547], [2778, 5547], [2778, 5547], [2778, 5547], [21424], [21424], [21424], [21424], [21424], [24987], [24987], [24987], [24987], [24987], [24987], [13209], [13209], [13209], [13209], [13209], [13209], [2927, 0], [2927, 0], [2927, 0], [2927, 0], [2927, 0], [2927, 0], [2927, 0], [1063], [1063], [1063], [1063], [1063], [1063], [1063], [154], [154], [154], [154], [154], [8733], [8733], [8733], [8733], [8733], [625, 7], [625, 7], [625, 7], [625, 7], [7697], [7697], [7697], [7697], [578], [578], [578], [578], [1421, 12], [1421, 12], [1421, 12], [1421, 12], [175, 250], [175, 250], [175, 250], [175, 250], [175, 250], [1283, 2], [1283, 2], [1283, 2], [1283, 2], [1283, 2], [5666], [5666], [5666], [5666], [5666], [5666], [441, 0], [441, 0], [441, 0], [441, 0], [441, 0], [441, 0], [441, 0], [15184, 7], [15184, 7], [15184, 7], [15184, 7], [15184, 7], [15184, 7], [15184, 7], [12849, 18], [12849, 18], [12849, 18], [12849, 18], [12849, 18], [12849, 18], [12849, 18], [12849, 18], [4371, 7], [4371, 7], [4371, 7], [4371, 7], [4371, 7], [4371, 7], [4371, 7], [4371, 7], [10215, 16], [10215, 16], [10215, 16], [10215, 16], [10215, 16], [10215, 16], [10215, 16], [10215, 16], [10215, 16], [4985, 12], [4985, 12], [4985, 12], [4985, 12], [4985, 12], [4985, 12], [4985, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)\n",
    "print(input_li_size)\n",
    "def generate_batch(iter, batch_size, input_li, output_li):\n",
    "    index = (iter % (input_li_size//batch_size)) * batch_size\n",
    "    batch_input = input_li[index:index+batch_size]\n",
    "    batch_output_li = output_li[index:index+batch_size]\n",
    "    batch_output = [[i] for i in batch_output_li]\n",
    "\n",
    "    return np.array(batch_input), np.array(batch_output)\n",
    "\n",
    "batch_inputs, batch_labels = generate_batch(0, batch_size, input_li, output_li)\n",
    "print(np.shape(batch_inputs))\n",
    "print(batch_inputs)\n",
    "print(np.shape(batch_labels))\n",
    "print(batch_labels)\n",
    "word_list = []\n",
    "for word in batch_inputs:\n",
    "    word_list.append(word_to_pos_dict[word])\n",
    "print(word_list)\n",
    "#     for pos in word_to_pos_dict[word]:\n",
    "#         print(pos)\n",
    "#         print(pos_reverse_dict[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-859141a89a0d>:49: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 150\n",
    "num_sampled = 50\n",
    "learning_rate = 1.0\n",
    "\n",
    "valid_size = 20     # Random set of words to evaluate similarity on.\n",
    "valid_window = 200  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False) # 200까지 숫자 중에서 랜덤하게 20개 뽑음\n",
    "\n",
    "# tensorflow 신경망 모델 그래프 생성\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    words_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(batch_size)] # batch_size만큼의 word를 형태소로\n",
    "    vocabulary_matrix = [tf.placeholder(tf.int32, shape=None) for _ in range(vocabulary_size)] # word_dict만큼의 word를 형태소로.. 인거 같은데 안씀\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # \"/device:GPU:0\"\n",
    "    with tf.device('/cpu:0'):\n",
    "        # embedding vector -> 우리가 원하는 최종 출력\n",
    "        pos_embeddings = tf.Variable(tf.random_uniform([pos_size, embedding_size], -1.0, 1.0), name='pos_embeddings')\n",
    "\n",
    "        word_vec_list = []\n",
    "        for i in range(batch_size):\n",
    "            word_vec = tf.reduce_sum(tf.nn.embedding_lookup(pos_embeddings, words_matrix[i]), 0)\n",
    "            word_vec_list.append(word_vec)\n",
    "        word_embeddings = tf.stack(word_vec_list) # word의 각 형태소를 embedding한 vector\n",
    "    \n",
    "        # Noise-Contrastive Estimation\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)), name='nce_weights'\n",
    "        )\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]), name='nce_biases')\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_labels,\n",
    "                       inputs=word_embeddings,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Compute the cosine similarity between minibatch exaples and all embeddings.\n",
    "    # 임의의 word로 유사도 검증\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(pos_embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = pos_embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iterations for each epoch : 133821\n",
      "Initialized - Tensorflow\n",
      "Average loss at step  0 :  216.94976806640625\n",
      "Nearest to ('에서', 'Noun'): ('넘겨주기', 'Noun'), ('하자', 'Verb'), ('어야만', 'Eomi'), ('경제성장률', 'Noun'), ('준우승', 'Noun'), ('베이가', 'Noun'), ('HANDS', 'Alpha'), ('싱크', 'Noun'),\n",
      "Nearest to ('키', 'Noun'): ('봉화', 'Noun'), ('담겨', 'Verb'), ('트루먼', 'Noun'), ('넓다', 'Adjective'), ('조정하는', 'Verb'), ('러시', 'Noun'), ('안정감', 'Noun'), ('해주는', 'Verb'),\n",
      "Nearest to ('에서는', 'Josa'): ('내일', 'Noun'), ('Adventure', 'Alpha'), ('실현되', 'Verb'), ('9000', 'Number'), ('엑스트라', 'Noun'), ('컴백', 'Noun'), ('제독', 'Noun'), ('생각되는', 'Verb'),\n",
      "Nearest to ('이었', 'Verb'): ('착하면', 'Adjective'), ('아테네', 'Noun'), ('커진', 'Verb'), ('소규모', 'Noun'), ('컵', 'Noun'), ('1813', 'Number'), ('온앤', 'Noun'), ('메모리즈', 'Noun'),\n",
      "Nearest to ('이', 'Determiner'): ('고시', 'Noun'), ('초록색', 'Noun'), ('흑사병', 'Noun'), ('Flower', 'Alpha'), ('맞춰', 'Verb'), ('내려온', 'Verb'), ('아르카디아', 'Noun'), ('이끌었', 'Verb'),\n",
      "Nearest to ('들', 'Suffix'): ('NCAA', 'Alpha'), ('캡틴', 'Noun'), ('Ex', 'Alpha'), ('소중한', 'Adjective'), ('순록', 'Noun'), ('하이퍼', 'Noun'), ('다이슨', 'Noun'), ('성공하기', 'Verb'),\n",
      "Nearest to ('에', 'Josa'): ('쌓는', 'Verb'), ('파동', 'Noun'), ('개', 'Verb'), ('고효준', 'Noun'), ('편이', 'Noun'), ('시트', 'Noun'), ('AC', 'Alpha'), ('평가하였', 'Verb'),\n",
      "Nearest to ('이나', 'Josa'): ('닿는', 'Verb'), ('괴랄', 'Noun'), ('과일', 'Noun'), ('부딪치는', 'Adjective'), ('비공개', 'Noun'), ('트랙', 'Noun'), ('불', 'Noun'), ('사족', 'Noun'),\n",
      "Nearest to ('자', 'Suffix'): ('전산', 'Noun'), ('비하인드', 'Noun'), ('특무', 'Noun'), ('지나가던', 'Verb'), ('거야', 'Eomi'), ('현직', 'Noun'), ('천명훈', 'Noun'), ('지기', 'Noun'),\n",
      "Nearest to ('100', 'Number'): ('win', 'Alpha'), ('펼쳐진', 'Verb'), ('1909', 'Number'), ('극복', 'Noun'), ('정장', 'Noun'), ('남겨', 'Verb'), ('김익렬', 'Noun'), ('><|', 'Punctuation'),\n",
      "Nearest to ('와', 'Noun'): ('needs', 'Alpha'), ('키우게', 'Verb'), ('더크', 'Noun'), ('emotion', 'Alpha'), ('갈아탔', 'Verb'), ('일하고', 'Verb'), ('하되', 'Noun'), ('Win', 'Alpha'),\n",
      "Nearest to ('적', 'Suffix'): ('Extreme', 'Alpha'), ('네이버', 'Noun'), ('가스피스톤', 'Noun'), ('보냈', 'Verb'), ('높아져', 'Verb'), ('카를', 'Noun'), ('니시무라', 'Noun'), ('자판기', 'Noun'),\n",
      "Nearest to ('20', 'Number'): ('실미도', 'Noun'), ('냉각', 'Noun'), ('tried', 'Alpha'), ('신세계', 'Noun'), ('거북이', 'Noun'), ('방대', 'Noun'), ('초청하여', 'Verb'), ('피아', 'Noun'),\n",
      "Nearest to ('스', 'Noun'): ('물어보았', 'Verb'), ('Age', 'Alpha'), ('Arsenal', 'Alpha'), ('는', 'Verb'), ('오딧세이', 'Noun'), ('Care', 'Alpha'), ('드래프트', 'Noun'), ('불과하고', 'Adjective'),\n",
      "Nearest to ('지만', 'Eomi'): ('Isolator', 'Alpha'), ('There', 'Alpha'), ('fix', 'Alpha'), ('코마츠', 'Noun'), ('증가하기', 'Verb'), ('지우기', 'Verb'), ('Discover', 'Alpha'), ('홀리', 'Noun'),\n",
      "Nearest to ('하기', 'Verb'): ('바깥', 'Noun'), ('몸매', 'Noun'), ('대미', 'Noun'), ('돌면', 'Verb'), ('초음속', 'Noun'), ('짤리', 'Noun'), ('범주', 'Noun'), ('지배하고', 'Verb'),\n",
      "Nearest to ('할', 'Verb'): ('information', 'Alpha'), ('출연자', 'Noun'), ('randomly', 'Alpha'), ('유동성', 'Noun'), ('스탠', 'Noun'), ('당연한', 'Adjective'), ('수입할', 'Verb'), ('착용', 'Noun'),\n",
      "Nearest to ('된', 'Verb'): ('마', 'Eomi'), ('흑의', 'Noun'), ('상승량', 'Noun'), ('로터리', 'Noun'), ('회전목마', 'Noun'), ('쫓겨나게', 'Verb'), ('그중', 'Adverb'), ('대응하고', 'Verb'),\n",
      "Nearest to ('점', 'Noun'): ('이지스', 'Noun'), ('처음', 'Noun'), ('지불하는', 'Verb'), ('Riot', 'Alpha'), ('isn', 'Alpha'), ('실현된', 'Verb'), ('카논', 'Noun'), ('정당성', 'Noun'),\n",
      "Nearest to ('>', 'Punctuation'): ('리미트', 'Noun'), ('더빙', 'Noun'), ('Billy', 'Alpha'), ('조속', 'Noun'), ('깔끔히', 'Adjective'), ('들어줄', 'Verb'), ('대운하', 'Noun'), ('종인', 'Noun'),\n",
      "Average loss at step  53528 :  634.4814699692643\n",
      "Average loss at step  107056 :  176.49446054399013\n",
      "Nearest to ('에서', 'Noun'): ('7', 'Number'), ('11', 'Number'), ('패턴', 'Noun'), ('9', 'Number'), ('당시', 'Noun'), ('이름', 'Noun'), ('등장하는', 'Verb'), ('버전', 'Noun'),\n",
      "Nearest to ('키', 'Noun'): ('dung', 'Alpha'), ('Na', 'Alpha'), ('⇒', 'Foreign'), ('Cling', 'Alpha'), ('발정기', 'Noun'), ('또봇', 'Noun'), ('나오는', 'Verb'), ('대부분', 'Noun'),\n",
      "Nearest to ('에서는', 'Josa'): ('에서도', 'Josa'), ('노여움', 'Noun'), ('뛰어난', 'Adjective'), ('과학기술', 'Noun'), ('끊지', 'Verb'), ('170', 'Number'), ('though', 'Alpha'), ('흉부', 'Noun'),\n",
      "Nearest to ('이었', 'Verb'): ('커진', 'Verb'), ('전투화', 'Noun'), ('본심', 'Noun'), ('혜택', 'Noun'), ('답한', 'Verb'), ('아테네', 'Noun'), ('더하여', 'Verb'), ('꿈꾸고', 'Verb'),\n",
      "Nearest to ('이', 'Determiner'): ('Steam', 'Alpha'), ('꺼', 'PreEomi'), ('전환하였', 'Verb'), ('Isma', 'Alpha'), ('못', 'PreEomi'), ('정체', 'Noun'), ('전재', 'Noun'), ('마르', 'Noun'),\n",
      "Nearest to ('들', 'Suffix'): ('흔적', 'Noun'), ('라트', 'Noun'), ('보컬리스트', 'Noun'), ('상표', 'Noun'), ('에게', 'Josa'), ('오파', 'Noun'), ('발달해', 'Verb'), ('다는', 'Eomi'),\n",
      "Nearest to ('에', 'Josa'): ('에서', 'Josa'), ('가', 'Josa'), ('의', 'Josa'), ('이', 'Josa'), ('은', 'Josa'), ('으로', 'Josa'), ('다', 'Eomi'), ('힘드므', 'Noun'),\n",
      "Nearest to ('이나', 'Josa'): ('cf', 'Alpha'), ('페인트', 'Noun'), ('다듬', 'Verb'), ('보다', 'Josa'), ('이루어졌', 'Verb'), ('제조', 'Noun'), ('DED', 'Alpha'), ('삼아', 'Verb'),\n",
      "Nearest to ('자', 'Suffix'): ('기질', 'Noun'), ('팔콘', 'Noun'), ('지원하여', 'Verb'), ('!)', 'Punctuation'), ('카레', 'Noun'), ('승자', 'Noun'), ('천명훈', 'Noun'), ('라이즈', 'Noun'),\n",
      "Nearest to ('100', 'Number'): ('착각할', 'Verb'), ('가능한', 'Adjective'), ('황무지', 'Noun'), ('Lite', 'Alpha'), ('참고', 'Noun'), ('장미란', 'Noun'), ('win', 'Alpha'), ('턴', 'Noun'),\n",
      "Nearest to ('와', 'Noun'): ('특히', 'Adverb'), ('attachment', 'Alpha'), ('small', 'Alpha'), ('아닌', 'Adjective'), ('범버', 'Noun'), ('_', 'Punctuation'), ('Cling', 'Alpha'), ('기존', 'Noun'),\n",
      "Nearest to ('적', 'Suffix'): ('웃겨', 'Verb'), ('해시', 'Noun'), ('피격', 'Noun'), ('케', 'Noun'), ('others', 'Alpha'), ('교단', 'Noun'), ('부풀', 'Noun'), ('나게', 'Verb'),\n",
      "Nearest to ('20', 'Number'): ('9', 'Number'), ('또봇', 'Noun'), ('Na', 'Alpha'), ('12', 'Number'), ('Encore', 'Alpha'), ('⇒', 'Foreign'), ('각각', 'Noun'), ('_', 'Punctuation'),\n",
      "Nearest to ('스', 'Noun'): ('Age', 'Alpha'), ('드래프트', 'Noun'), ('오딧세이', 'Noun'), ('실기', 'Noun'), ('듣', 'Verb'), ('천문학', 'Noun'), ('단순한', 'Adjective'), ('Diver', 'Alpha'),\n",
      "Nearest to ('지만', 'Eomi'): ('게', 'Noun'), ('편이', 'Noun'), ('잘', 'Verb'), ('건', 'Noun'), ('데', 'Noun'), ('WANT', 'Alpha'), ('많이', 'Adverb'), ('다만', 'Noun'),\n",
      "Nearest to ('하기', 'Verb'): ('attachment', 'Alpha'), ('함께', 'Adverb'), ('png', 'Alpha'), ('Na', 'Alpha'), ('_', 'Punctuation'), ('특히', 'Adverb'), ('위해', 'Noun'), ('하며', 'Verb'),\n",
      "Nearest to ('할', 'Verb'): ('볼', 'Noun'), ('Na', 'Alpha'), ('될', 'Verb'), ('있고', 'Adjective'), ('⇒', 'Foreign'), ('알', 'Noun'), ('하고', 'Verb'), ('그러나', 'Conjunction'),\n",
      "Nearest to ('된', 'Verb'): ('되는', 'Verb'), ('small', 'Alpha'), ('Na', 'Alpha'), ('attachment', 'Alpha'), ('Cling', 'Alpha'), ('_', 'Punctuation'), ('png', 'Alpha'), ('되', 'Verb'),\n",
      "Nearest to ('점', 'Noun'): ('Na', 'Alpha'), ('Cling', 'Alpha'), ('small', 'Alpha'), ('⇒', 'Foreign'), ('png', 'Alpha'), ('또봇', 'Noun'), ('그러나', 'Conjunction'), ('상황', 'Noun'),\n",
      "Nearest to ('>', 'Punctuation'): ('Na', 'Alpha'), ('nous', 'Alpha'), ('같은', 'Adjective'), ('스케줄링', 'Noun'), ('⇒', 'Foreign'), ('small', 'Alpha'), ('될', 'Verb'), ('범버', 'Noun'),\n",
      "Average loss at step  160584 :  148.05313230853156\n",
      "Average loss at step  214112 :  143.77907384249195\n"
     ]
    }
   ],
   "source": [
    "num_iterations = input_li_size // batch_size\n",
    "print(\"number of iterations for each epoch :\", num_iterations)\n",
    "epochs = 4\n",
    "num_steps = num_iterations * epochs + 1\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    init.run()\n",
    "    print(\"Initialized - Tensorflow\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(step, batch_size, input_li, output_li)\n",
    "\n",
    "        word_list = []\n",
    "        for word in batch_inputs:\n",
    "            word_list.append(word_to_pos_dict[word])\n",
    "\n",
    "        feed_dict = {}\n",
    "        for i in range(batch_size):\n",
    "            feed_dict[words_matrix[i]] = word_list[i]\n",
    "        feed_dict[train_inputs] = batch_inputs\n",
    "        feed_dict[train_labels] = batch_labels\n",
    "\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % (num_steps//10) == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        if step % (num_steps//4) == 0:\n",
    "            pos_embed = pos_embeddings.eval()\n",
    "\n",
    "            # Print nearest words\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_pos = pos_reverse_dict[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % str(valid_pos)\n",
    "                for k in range(top_k):\n",
    "                    close_word = pos_reverse_dict[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, str(close_word))\n",
    "                print(log_str)\n",
    "\n",
    "    pos_embed = pos_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save vectors.\n",
    "def save_model(pos_list, embeddings, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(str(len(pos_list)))\n",
    "        f.write(\" \")\n",
    "        f.write(str(embedding_size))\n",
    "        f.write(\"\\n\")\n",
    "        for i in range(len(pos_list)):\n",
    "            pos = pos_list[i]\n",
    "            f.write(str(pos).replace(\"', '\", \"','\") + \" \")\n",
    "            f.write(' '.join(map(str, embeddings[i])))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Save vectors\n",
    "save_model(pos_li, pos_embed, \"pos.vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pk_story",
   "language": "python",
   "name": "pk_story"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
